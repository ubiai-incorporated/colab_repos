{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfAQB77cnLfh"
      },
      "source": [
        "## Installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2rynmanL2y"
      },
      "outputs": [],
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "! pip install -q git+https://github.com/huggingface/datasets.git \"dill<0.3.5\" seqeval\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpb9Df-XnZCZ"
      },
      "source": [
        "## Pulling preprocessing file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tv1cRSLnTyh"
      },
      "outputs": [],
      "source": [
        "! rm -r layoutlmv3FineTuning\n",
        "! git clone -b master https://github.com/ubiai-incorporated/layoutlm-preprocess.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3L0nDhP-nl_q"
      },
      "source": [
        "## Loading UBIAI dataset from Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export your data from UbiAI in \"OCR Processed Format\"make sure to place the exported zip in /content/"
      ],
      "metadata": {
        "id": "i5FBxJ7n5NRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -r data"
      ],
      "metadata": {
        "id": "itUkhhhYw_3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQnYjbS8nnXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e8e5452-23ed-43bf-a659-ca2ed315366a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'data': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#!/bin/bash\n",
        "IOB_DATA_PATH = \"/content/drive/MyDrive/Invoice Dataset/exported_data.zip\"\n",
        "! cd /content/\n",
        "! rm -r data\n",
        "! mkdir data\n",
        "! cp \"$IOB_DATA_PATH\" data/dataset.zip\n",
        "! cd data && unzip -q dataset && rm dataset.zip\n",
        "! cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob4u_OHvNF2D",
        "outputId": "ba3b3a08-1305-4c35-b636-e912adb5bd68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "data_folder = '/content/data'\n",
        "files = os.listdir(data_folder)\n",
        "pattern = r'([a-f0-9\\-]+)(?=_\\w+\\.txt)'\n",
        "common_string = None\n",
        "\n",
        "for file in files:\n",
        "    match = re.search(pattern, file)\n",
        "    if match:\n",
        "        common_string = match.group(1)\n",
        "        break\n",
        "\n",
        "if common_string:\n",
        "    original_zip_path = '/drive/MyDrive/Invoice Dataset/exported_date.zip'  # Original ZIP file path\n",
        "    new_zip_path = f'/content/{common_string}.zip'  # New ZIP file path\n",
        "    os.rename(original_zip_path, new_zip_path)\n",
        "    print(f'Renamed zip file to {new_zip_path}')\n",
        "else:\n",
        "    print('Could not find the common string pattern in the text files')\n",
        "\n"
      ],
      "metadata": {
        "id": "EsO9XTJzznKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "then run this cell to create the output folder where the processed dataset will be saved"
      ],
      "metadata": {
        "id": "-UvQzMOJ0yHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "output_folder = '/content/output'\n",
        "\n",
        "subfolders = ['train_split', 'test_split', 'raw_data']\n",
        "if not os.path.exists(output_folder):\n",
        "    os.mkdir(output_folder)\n",
        "for subfolder in subfolders:\n",
        "    path = os.path.join(output_folder, subfolder)\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)"
      ],
      "metadata": {
        "id": "2U63aEOkzfwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cn8X5s4n8t5"
      },
      "source": [
        "## defining preprocessing params and running the script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_-H4goBn-F7"
      },
      "outputs": [],
      "source": [
        "#!/bin/bash\n",
        "#preprocessing args\n",
        "TEST_SIZE = 0.33\n",
        "DATA_OUTPUT_PATH = \"/content/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the preprocessing script"
      ],
      "metadata": {
        "id": "48kqnXwa9FnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vBFmQ4hoD2p"
      },
      "outputs": [],
      "source": [
        "! python3 layoutlm-preprocess/preprocess.py --valid_size $TEST_SIZE --output_path $DATA_OUTPUT_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training"
      ],
      "metadata": {
        "id": "PmddlmGWpv_3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzJbXg4gpM3B"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import LayoutLMv3ForTokenClassification,AutoProcessor\n",
        "from transformers.data.data_collator import default_data_collator\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU75MUSsohIb"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "from datasets import load_from_disk\n",
        "train_dataset = load_from_disk('/content/output/train_split')\n",
        "eval_dataset = load_from_disk('/content/output/test_split')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEDj23OQHjlR"
      },
      "outputs": [],
      "source": [
        "label_list = train_dataset.features[\"labels\"].feature.names\n",
        "num_labels = len(label_list)\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(label_list):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(train_dataset)\n",
        "print(f\"Dataset size: {dataset_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQBjXq6hzWBg",
        "outputId": "be8ed438-8cc5-42d7-ab4b-386f8fa6cd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_list)"
      ],
      "metadata": {
        "id": "qssyNBDOxvRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hBZo3FhoUEJ"
      },
      "source": [
        "## defining metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RptozkCioxhB"
      },
      "outputs": [],
      "source": [
        "metric = load_metric(\"seqeval\")\n",
        "import numpy as np\n",
        "\n",
        "return_entity_level_metrics = False\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Remove ignored index (special tokens)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    results = metric.compute(predictions=true_predictions, references=true_labels,zero_division='0')\n",
        "    if return_entity_level_metrics:\n",
        "        # Unpack nested dictionaries\n",
        "        final_results = {}\n",
        "        for key, value in results.items():\n",
        "            if isinstance(value, dict):\n",
        "                for n, v in value.items():\n",
        "                    final_results[f\"{key}_{n}\"] = v\n",
        "            else:\n",
        "                final_results[key] = value\n",
        "        return final_results\n",
        "    else:\n",
        "        return {\n",
        "            \"precision\": results[\"overall_precision\"],\n",
        "            \"recall\": results[\"overall_recall\"],\n",
        "            \"f1\": results[\"overall_f1\"],\n",
        "            \"accuracy\": results[\"overall_accuracy\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VxWc2tGo8R4"
      },
      "source": [
        "## loading model and preprocessor (also required for Hugging face trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLcWS0BrpCp0"
      },
      "outputs": [],
      "source": [
        "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\",\n",
        "                                                         id2label=id2label,\n",
        "                                                         label2id=label2id)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmqcO0gJpSD5"
      },
      "source": [
        "## let's train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_TRAIN_EPOCHS = 120 #increase this to your liking\n",
        "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
        "PER_DEVICE_EVAL_BATCH_SIZE = 4\n",
        "LEARNING_RATE = 4e-5"
      ],
      "metadata": {
        "id": "VNYR31ydPMic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iInYrU6DpD5p"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(output_dir=\"output\",\n",
        "                                  # max_steps=1500,\n",
        "                                  num_train_epochs=NUM_TRAIN_EPOCHS,\n",
        "                                  logging_strategy=\"epoch\",\n",
        "                                  save_total_limit=1,\n",
        "                                  per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "                                  per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "                                  learning_rate=LEARNING_RATE,\n",
        "                                  evaluation_strategy=\"no\",\n",
        "                                  save_strategy=\"no\",\n",
        "                                  dataloader_pin_memory=False,\n",
        "                                  # eval_steps=100,\n",
        "                                  load_best_model_at_end=True,\n",
        "                                  metric_for_best_model=\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O0rFEiBrGpQ"
      },
      "outputs": [],
      "source": [
        "# Initialize our Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=processor,\n",
        "    data_collator=default_data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUL9WBIopT3J"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBE4iLXtpVS5"
      },
      "source": [
        "save the model to a folder output you create"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model,'/content/model_output/layoutlmv3.pth')"
      ],
      "metadata": {
        "id": "bTbp-i9krEoZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}